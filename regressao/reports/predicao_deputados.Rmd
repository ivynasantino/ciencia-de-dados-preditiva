---
title: "Predição de deputados eleitos"
author: "Ivyna Santino"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
  html_notebook:
    toc: yes
    toc_float: yes
---

### Introdução
Para esse checkpoint iremos utilizar conhecimentos sobre classificação para prever quais candidatos à Câmara de Deputados serão eleitos nas eleições de 2014. 

```{r setup, warning=FALSE, message=FALSE, include=FALSE}
library(tidyverse)
library(here)
library(caret)
library(rpart)

theme_set(theme_minimal())
```

Importando os dados de treino:
```{r, message=FALSE, warning=FALSE}
train <- read.csv(here("data/situacao/train.csv"))
```
Tratando os dados:

Para essa etapa, retirei a maioria das variáveis categóricas dos dados de treino. O único parâmetro categórico que permaneceu no nosso dataframe foi o de ocupação, que posteriomente foi transformado em fator para que possamos realizar as operações de regressão.

```{r, warning=FALSE, message=FALSE}
train <- train %>% 
  select(-cargo,
         -sequencial_candidato,
         -nome,
         -uf,
         -estado_civil,
         -grau,
         -ano,
         -sexo,
         -partido)
```


```{r, warning=FALSE, message=FALSE}
train$ocupacao <- as.factor(train$ocupacao)
```

### 1.Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador? Como você poderia tratar isso?


```{r, warning=FALSE, message=FALSE}
quantidade_situacao <- table(train$situacao)
quantidade_situacao
```

```{r, warning=FALSE, message=FALSE}
proporcaoEleito <- quantidade_situacao[1]/(quantidade_situacao[1] + quantidade_situacao[2])
proporcaoEleito
```
```{r, warning=FALSE, message=FALSE}
proporcaoNEleito <- quantidade_situacao[2]/(quantidade_situacao[1] + quantidade_situacao[2])
proporcaoNEleito
```

Temos um desbalanceamento dos dados com relação a quantidade de parlamentares eleitos e não eleitos, como podemos ver acima, com proporção de 13% para eleitos e 86% para não eleitos. Dessa maneira, com este cenário de desbalanceamento de classes o nosso classificador poderá dar resultados mais precisos para classes com maior proporção e errar quanto aos resultados das classes que possuem menor proporção de dados. Por isso, podemos tratar isso com parâmentros de balanceamento na validação cruzada. 

### 2.Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo.  

#### Validação cruzada

```{r, warning=FALSE, message=FALSE}
#VC <- trainControl(method = "repeatedcv",
#                    number = 5,
#                    repeats = 5,
#                    sampling = "smote",
#                    verboseIter = TRUE)
```

#### Regressão logística

```{r, warning=FALSE, message=FALSE}
# modelo <- train(situacao ~ .,
#                 data = train,
#                 method = "regLogistic",
#                 trControl = VC)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
#saveRDS(modelo, file="modelo.rds")
modelo <- readRDS(file="modelo.rds")
```

```{r, warning=FALSE, message=FALSE}
modelo
```

```{r, warning=FALSE, message=FALSE}
confusion.rl <- confusionMatrix(modelo)
confusion.rl
```

#### Knn

```{r, warning=FALSE, message=FALSE}
# modelo.knn <- train(situacao ~ .,
#                 data = train,
#                 method = "knn",
#                 trControl = VC)
```


```{r, warning=FALSE, message=FALSE, include=FALSE}
#saveRDS(modelo.knn, file="modelo_knn.rds")
modelo.knn <- readRDS(file="modelo_knn.rds")
```


```{r, warning=FALSE, message=FALSE}
modelo.knn
```

```{r, warning=FALSE, message=FALSE}
confusion.knn <- confusionMatrix(modelo.knn)
confusion.knn
```

#### Árvore de decisão

```{r, warning=FALSE, message=FALSE}
# modelo.dt <- train(situacao ~ .,
#                 data = train,
#                 method = "rpart",
#                 trControl = VC)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
#saveRDS(modelo.dt, file="modelo_dt.rds")
modelo.dt <- readRDS(file="modelo_dt.rds")
```

```{r, warning=FALSE, message=FALSE}
modelo.dt
```

```{r, warning=FALSE, message=FALSE}
confusion.dt <- confusionMatrix(modelo.dt)
confusion.dt
```
#### Adaboost

```{r, warning=FALSE, message=FALSE}
# modelo.ada <- train(situacao ~ .,
#                 data = train,
#                 method = "adaboost",
#                 trControl = VC)
```

```{r, warning=FALSE, message=FALSE, include=FALSE}
#saveRDS(modelo.ada, file="modelo_ada.rds")
modelo.ada <- readRDS(file="modelo_ada.rds")
```


```{r, warning=FALSE, message=FALSE}
modelo.ada
```


```{r, warning=FALSE, message=FALSE}
confusion.ada <- confusionMatrix(modelo.ada)
confusion.ada
```

### 3.Reporte acurácia, precision, recall e f-measure no treino e validação. Há uma grande diferença de desempenho no treino/validação? Como você avalia os resultados? Justifique sua resposta.

Sumário das variáveis:
  vp: verdadeiros positivos - table[1]
  fp: falsos positivos - table[3]
  vn: verdadeiros negativos - table[2]
  fn: falsos negativos - table[4]

```{r, warning=FALSE, message=FALSE}
precision <- function(vp, fp) {
  resultado = vp/(vp + fp)
  return(resultado)
}

recall <- function(vp, fn) {
  resultado = vp/(vp + fn)
  return(resultado)
}

f_measure <- function(p, r) {
  resultado = 2*((p * r)/(p + r))
  return(resultado)
}

acuracia <- function(vp, fp, vn, fn) {
  resultado = (vp + fn)/(vp + fp + vn + fn)
  return(resultado)
}
```


Obs.: O que significa cada parâmentro modularizado acima:

  - Precision: proporção de predições positivas que são realmente positivas.
  - Recall: proporção de predições que foram corretamente classificadas.
  - Acurácia: proporção de predições corretamente classificadas, também é dada pela matriz de confusão calculada nos passos anteriores.

#### Regressão logística

```{r, warning=FALSE, message=FALSE}
precision.rl <- precision(confusion.rl$table[1], confusion.rl$table[3])
recall.rl <- recall(confusion.rl$table[1], confusion.rl$table[2])
f_measure.rl <- f_measure(precision.rl, recall.rl)
acuracia.rl <- acuracia(confusion.rl$table[1], 
                        confusion.rl$table[3], 
                        confusion.rl$table[2], 
                        confusion.rl$table[4])
```

```{r, warning=FALSE, message=FALSE}
precision.rl
recall.rl
f_measure.rl
acuracia.rl
```

#### Knn

```{r, warning=FALSE, message=FALSE}
precision.knn <- precision(confusion.knn$table[1], confusion.knn$table[3])
recall.knn <- recall(confusion.knn$table[1], confusion.knn$table[2])
f_measure.knn <- f_measure(precision.knn, recall.knn)
acuracia.knn <- acuracia(confusion.knn$table[1], 
                        confusion.knn$table[3], 
                        confusion.knn$table[2], 
                        confusion.knn$table[4])
```

```{r, warning=FALSE, message=FALSE}
precision.knn
recall.knn
f_measure.knn
acuracia.knn
```

#### Árvore de decisão

```{r, warning=FALSE, message=FALSE}
precision.dt <- precision(confusion.dt$table[1], confusion.dt$table[3])
recall.dt <- recall(confusion.dt$table[1], confusion.dt$table[2])
f_measure.dt <- f_measure(precision.dt, recall.dt)
acuracia.dt <- acuracia(confusion.dt$table[1], 
                        confusion.dt$table[3], 
                        confusion.dt$table[2], 
                        confusion.dt$table[4])
```

```{r, warning=FALSE, message=FALSE}
precision.dt
recall.dt
f_measure.dt
acuracia.dt
```

#### Adaboost

```{r, warning=FALSE, message=FALSE}
precision.ada <- precision(confusion.ada$table[1], confusion.ada$table[3])
recall.ada <- recall(confusion.ada$table[1], confusion.ada$table[2])
f_measure.ada <- f_measure(precision.ada, recall.ada)
acuracia.ada <- acuracia(confusion.ada$table[1], 
                        confusion.ada$table[3], 
                        confusion.ada$table[2], 
                        confusion.ada$table[4])
```

```{r, warning=FALSE, message=FALSE}
precision.ada
recall.ada
f_measure.ada
acuracia.ada
```

Como podemos ver nos resultados acima, todos os modelos que construímos tiveram resultados satisfatórios, principalmente para a acurácia, vista também nos valores acima. Mas queria destacar os valores do Adaboost, que obteve acurácia 0.9 se enquadrando no melhor modelo construído entre os demais desse checkpoint, fora acurácia o valor de recall também foi alto, que significa que sua previsão consegue classificar corretamente, cerca de 0.85.


### 4.Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo

Para definirmos qual atributo criar, vamos observar qual a importância de cada um no modelo.

```{r, warning=FALSE, message=FALSE}
varImp(modelo.dt)
```

Assim como nos checkpoints anteriores, podemos ver que total_receita e total_despesa tiveram resultados bem altos. Então que tal criarmos uma variável de gastos através da diferença entre receita e despesa?  

Antes disso, criei um novo dataframe com os dados de treino.
```{r, message=FALSE, warning=FALSE}
train_novo <- read.csv(here("data/situacao/train.csv"))
```

Criando atributos:

```{r, warning=FALSE, message=FALSE}
train_novo$gastos <- train_novo$total_receita - train_novo$total_despesa
```

Além dos gastos, acredito que parlamentares que já são deputados tem vantagens para serem eleitos novamente, dessa maneira criei um atributo que verifica se o candidato tem como ocupação deputado ou não.

```{r, warning=FALSE, message=FALSE}
train_novo$deputado <- ifelse(train_novo$ocupacao == "DEPUTADO", TRUE, FALSE)

train_novo$deputado <- as.factor(train_novo$deputado)
```

Outra característica que acredito que faça influência é se o candidato tem ensino superior.
```{r, warning=FALSE, message=FALSE}
train_novo$ens_superior <- ifelse(train_novo$grau == "SUPERIOR COMPLETO", TRUE, FALSE)

train_novo$ens_superior <- as.factor(train_novo$ens_superior)
```

Como os atributos que criei, em maioria, são categóricos tive que transformá-los em fator também. Da mesma forma que fiz anteriormente, retirei as colunas com parâmentros categóricos e dessa vez retirei mais um atributo(ocupação), por dois motivos: como era categórica e mesmo transformando em fator, no momento de rodar as regressões ficava muito demorado e o Rstudio acabava fechando e o outro motivo é que fiz a coluna deputado.

```{r, warning=FALSE, message=FALSE}
train_novo <- train_novo %>% 
  select(-cargo,
         -sequencial_candidato,
         -nome,
         -uf,
         -estado_civil,
         -grau,
         -ano,
         -sexo,
         -partido,
         -ocupacao)
```

Escolhi os dois melhores modelos de acordo com os resultados anteriores: árvore de decisão e o adaboost.

#### Árvore de decisão

```{r, warning=FALSE, message=FALSE}
# modelo.dt.novo <- train(situacao ~ .,
#                 data = train_novo,
#                 method = "rpart",
#                 trControl = VC)
```

```{r, warning=FALSE, message=FALSE}
#saveRDS(modelo.dt.novo, file="modelo_dt_novo.rds")
modelo.dt.novo <- readRDS(file="modelo_dt_novo.rds")
```

```{r, warning=FALSE, message=FALSE}
modelo.dt.novo
```

```{r, warning=FALSE, message=FALSE}
confusion.dt.novo <- confusionMatrix(modelo.dt.novo)
confusion.dt.novo
```

#### Adaboost

```{r, warning=FALSE, message=FALSE}
# modelo.ada.novo <- train(situacao ~ .,
#                 data = train_novo,
#                 method = "adaboost",
#                 trControl = VC)
```

```{r, warning=FALSE, message=FALSE}
#saveRDS(modelo.ada.novo, file="modelo_ada_novo.rds")
modelo.ada.novo <- readRDS(file="modelo_ada_novo.rds")
```

```{r, warning=FALSE, message=FALSE}
modelo.ada.novo
```

```{r, warning=FALSE, message=FALSE}
confusion.ada.novo <- confusionMatrix(modelo.ada.novo)
confusion.ada.novo
```

Como podemos ver nossos modelos tiveram uma leve queda na acurácia, acredito que seja pelo fator da retirada do atributo de ocupação. Mas ainda continuam modelos muito bons.

### 5.Envie seus melhores modelos à competição do Kaggle. Faça pelo menos uma submissão. 
  
Sugestões para melhorar o modelo: 
  Experimente outros modelos (e.g. SVM, RandomForests e GradientBoosting).
  Experimente balancear as classes,  caso estejam desbalanceadas.
  Crie pelo menos um novo atributo.

```{r, message=FALSE, warning=FALSE}
test = read.csv(here("data/situacao/test.csv"))
```

```{r, warning=FALSE, message=FALSE}
submissao <- test %>% 
  select(sequencial_candidato)

```

```{r, warning=FALSE, message=FALSE}
test <- test %>% 
  select(-cargo,
         -sequencial_candidato,
         -nome,
         -uf,
         -estado_civil,
         -grau,
         -ano,
         -sexo,
         -partido)
```

```{r, warning=FALSE, message=FALSE}
predicao <- predict(modelo.ada, test)
submissao$Predicted <- predicao

submissao <- submissao %>% 
  select(ID = sequencial_candidato,
         Predicted = Predicted)
```

```{r, warning=FALSE, message=FALSE}
write.csv(x = submissao,
          file = "../data/situacao/submissao.csv",
          row.names = FALSE)


```


