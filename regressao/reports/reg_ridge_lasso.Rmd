---
title: "Predição de votação dos deputados"
output: html_notebook
---

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(caret)

theme_set(theme_minimal())
```

Descrição do problema:

- Usando todas as variáveis disponíveis, tune (usando validação cruzada): 
  (i) um modelo de regressão Ridge ok
  (ii) um modelo de regressão Lasso ok
  (iii) um modelo KNN. Para os modelos de regressão linear, o parâmetro a ser tunado é o lambda (penalização dos coeficientes) e o KNN o número de vizinhos. (9 pts.) ok

- Compare os três modelos em termos do erro RMSE de validação cruzada. (9 pts.)
- Quais as variáveis mais importantes segundo o modelo de regressão Ridge e Lasso?  Variáveis foram descartadas pelo Lasso? Quais? (9 pts.) ok
- Re-treine o melhor modelo (usando os melhores valores de parâmetros encontrados em todos os dados, sem usar validação cruzada). (9 pts.) ok
- Use esse último modelo treinado para prever os dados de teste disponíveis no challenge que criamos na plataforma Kaggle (Links para um site externo)
Links para um site externo (9 pts.)

```{r, warning=FALSE, message=FALSE}
importa_eleicao <- function(dataset_path) {
  read = read.csv(here(paste("data/all", dataset_path, sep = "/")))
  return(read)
}

```

```{r, warning=FALSE, message=FALSE}
train = importa_eleicao("train.csv")
test = importa_eleicao("test.csv")
```



```{r, warning=FALSE, message=FALSE}
train <- train %>% 
  select(-cargo,
         -sequencial_candidato,
         -ocupacao,
         -uf,
         -nome)
```


```{r, warning=FALSE, message=FALSE}

modelo_ridge <- train(votos ~ .,
                     data = train,
                     method = "ridge")


modelo_lasso <- train(votos ~ .,
                     data = train,
                     
                     method = "lasso")


modelo_knn <- train(votos ~ .,
                     data = train,
                     method = "knn")

```



```{r, warning=FALSE, message=FALSE}
modelo_ridge
# 
# Ridge Regression 
# 
# 7476 samples
#   18 predictor
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   lambda  RMSE          Rsquared   MAE         
#   0e+00   1.813757e+12  0.3355068  3.528742e+10
#   1e-04   3.937651e+04  0.3651657  1.644839e+04
#   1e-01   4.014603e+04  0.3592090  1.643595e+04
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 1e-04.

```


```{r, warning=FALSE, message=FALSE}
modelo_lasso
# 
# The lasso 
# 
# 7476 samples
#   18 predictor
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   fraction  RMSE      Rsquared   MAE     
#   0.1       42667.20  0.3486232  17703.69
#   0.5       65992.53  0.3013075  17097.56
#   0.9       91408.60  0.2962026  17648.44
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was fraction = 0.1.

```

```{r, warning=FALSE, message=FALSE}
modelo_knn
# 
# k-Nearest Neighbors 
# 
# 7476 samples
#   18 predictor
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   k  RMSE      Rsquared   MAE     
#   5  36436.60  0.4381170  13778.84
#   7  35221.21  0.4650971  13295.98
#   9  34604.39  0.4788859  13016.16
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 9.

```



```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_ridge)) +
geom_col(fill = "#F08080") +
labs(title = "Importância das variáveis do modelo Ridge",
     y = "Importância",
     x = "Variável(s)")
```

```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_lasso)) +
geom_col(fill = "#FFA07A") +
labs(title = "Importância das variáveis do modelo Lasso",
     y = "Importância",
     x = "Variável(s)")
```


```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_knn)) +
geom_col(fill = "#F4A460") +
labs(title = "Importância das variáveis do modelo Knn",
     y = "Importância",
     x = "Variável(s)")
  
```

Selecionando as variáveis de acordo com os gráficos acima:

total_receita, total_despesa, recursos_de_pessoas_juridicas


```{r, warning=FALSE, message=FALSE}
modelo_select <- train %>% 
  select(total_receita,
         total_despesa,
         recursos_de_pessoas_juridicas,
         votos)
```


```{r, warning=FALSE, message=FALSE}
modelo_select_ridge <- train(votos ~ .,
                     data = modelo_select,
                     method = "ridge")

modelo_select_lasso <- train(votos ~ .,
                     data = modelo_select,
                     method = "lasso")

modelo_select_knn <- train(votos ~ .,
                     data = modelo_select,
                     method = "knn")

```


```{r, warning=FALSE, message=FALSE}
modelo_select_ridge
# 
# Ridge Regression 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   lambda  RMSE      Rsquared   MAE     
#   0e+00   36943.41  0.3948709  17646.60
#   1e-04   36943.18  0.3948734  17647.59
#   1e-01   37192.25  0.3874505  17540.41
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 1e-04.

```


```{r, warning=FALSE, message=FALSE}
modelo_select_lasso
# 
# The lasso 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   fraction  RMSE     Rsquared   MAE     
#   0.1       43632.6  0.3755933  24930.51
#   0.5       37904.7  0.3789884  17850.19
#   0.9       37816.1  0.3815954  17628.47
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was fraction = 0.9.

```


```{r, warning=FALSE, message=FALSE}
modelo_select_knn
# 
# k-Nearest Neighbors 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   k  RMSE      Rsquared   MAE     
#   5  37975.11  0.4046912  14366.88
#   7  36573.01  0.4329369  13947.41
#   9  35643.31  0.4509839  13642.85
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 9.

```


Fazendo modelos com as variáveis escolhidas no checkpoint anterior:

cola:

qnt_fornec - qtd_desp = 0.99
qnt_desp - tot_receita = 0.76
pes_jur - tot-receita = 0.85
pes_jur - tot_desp = 0.84
tot_receita - tot_desp = 0.99
qtd_doacoes - qtd_doadores = 1

com votos
tot_desp = 0.61
tot_receita = 0.6
pes_juridica = 0.55

escolha para modelo: 
tot_desp
qnt_forn
qtd_doadores

