---
title: "Predição de votação dos deputados"
output: html_notebook
---

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(caret)

theme_set(theme_minimal())
```

Descrição do problema:

- Usando todas as variáveis disponíveis, tune (usando validação cruzada): 
  (i) um modelo de regressão Ridge ok
  (ii) um modelo de regressão Lasso ok
  (iii) um modelo KNN. Para os modelos de regressão linear, o parâmetro a ser tunado é o lambda (penalização dos coeficientes) e o KNN o número de vizinhos. (9 pts.) ok

- Compare os três modelos em termos do erro RMSE de validação cruzada. (9 pts.)
- Quais as variáveis mais importantes segundo o modelo de regressão Ridge e Lasso?  Variáveis foram descartadas pelo Lasso? Quais? (9 pts.) ok
- Re-treine o melhor modelo (usando os melhores valores de parâmetros encontrados em todos os dados, sem usar validação cruzada). (9 pts.) ok
- Use esse último modelo treinado para prever os dados de teste disponíveis no challenge que criamos na plataforma Kaggle (Links para um site externo)
Links para um site externo (9 pts.)

```{r, warning=FALSE, message=FALSE}
importa_eleicao <- function(dataset_path) {
  read = read.csv(here(paste("data/all", dataset_path, sep = "/")))
  return(read)
}

```

```{r, warning=FALSE, message=FALSE}
train = importa_eleicao("train.csv")
test = importa_eleicao("test.csv")
```



```{r, warning=FALSE, message=FALSE}
train <- train %>% 
  select(-cargo,
         -sequencial_candidato,
         -ocupacao,
         -uf,
         -nome)
```


```{r, warning=FALSE, message=FALSE}

modelo_ridge <- train(votos ~ .,
                     data = train,
                     method = "ridge")


modelo_lasso <- train(votos ~ .,
                     data = train,
                     
                     method = "lasso")


modelo_knn <- train(votos ~ .,
                     data = train,
                     method = "knn")

```



```{r, warning=FALSE, message=FALSE}
modelo_ridge
# Ridge Regression 
# 
# 7476 samples
#   18 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   lambda  RMSE      Rsquared   MAE     
#   0e+00   56515.38  0.3442496  16932.85
#   1e-04   37477.05  0.3962081  16466.81
#   1e-01   38050.98  0.3902056  16440.98
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 1e-04.
```


```{r, warning=FALSE, message=FALSE}
modelo_lasso

# The lasso 
# 
# 7476 samples
#   18 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   fraction  RMSE      Rsquared   MAE     
#   0.1       41296.23  0.3360462  17264.30
#   0.5       58228.20  0.2717016  16931.75
#   0.9       78294.50  0.2600462  17345.89
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was fraction = 0.1.

```

```{r, warning=FALSE, message=FALSE}
modelo_knn

# k-Nearest Neighbors 
# 
# 7476 samples
#   18 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   k  RMSE      Rsquared   MAE     
#   5  37343.24  0.4268117  13959.22
#   7  35736.20  0.4583032  13460.61
#   9  34970.26  0.4753359  13182.20
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 9.

```



```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_ridge)) +
geom_col(fill = "#F08080") +
labs(title = "Importância das variáveis do modelo Ridge",
     y = "Importância",
     x = "Variável(is)")
```

```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_lasso)) +
geom_col(fill = "#FFA07A") +
labs(title = "Importância das variáveis do modelo Lasso",
     y = "Importância",
     x = "Variável(is)")
```


```{r, warning=FALSE, message=FALSE}
ggplot(varImp(modelo_knn)) +
geom_col(fill = "#F4A460") +
labs(title = "Importância das variáveis do modelo Knn",
     y = "Importância",
     x = "Variável(is)")
  
```

Selecionando as variáveis de acordo com os gráficos acima:

- total_receita
- total_despesa
- recursos_de_pessoas_juridicas


```{r, warning=FALSE, message=FALSE}
modelo_select <- train %>% 
  select(total_receita,
         total_despesa,
         recursos_de_pessoas_juridicas,
         votos)
```


```{r, warning=FALSE, message=FALSE}
modelo_select_ridge <- train(votos ~ .,
                     data = modelo_select,
                     method = "ridge")

modelo_select_lasso <- train(votos ~ .,
                     data = modelo_select,
                     method = "lasso")

modelo_select_knn <- train(votos ~ .,
                     data = modelo_select,
                     method = "knn")

```


```{r, warning=FALSE, message=FALSE}
modelo_select_ridge
# 
# Ridge Regression 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   lambda  RMSE      Rsquared   MAE     
#   0e+00   36295.67  0.3992942  17539.31
#   1e-04   36295.39  0.3993021  17540.27
#   1e-01   36538.17  0.3939808  17424.94
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 1e-04.

```


```{r, warning=FALSE, message=FALSE}
modelo_select_lasso
# 
# The lasso 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   fraction  RMSE     Rsquared   MAE     
#   0.1       43632.6  0.3755933  24930.51
#   0.5       37904.7  0.3789884  17850.19
#   0.9       37816.1  0.3815954  17628.47
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was fraction = 0.9.

```


```{r, warning=FALSE, message=FALSE}
modelo_select_knn
# 
# k-Nearest Neighbors 
# 
# 7476 samples
#    3 predictors
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   k  RMSE      Rsquared   MAE     
#   5  37975.11  0.4046912  14366.88
#   7  36573.01  0.4329369  13947.41
#   9  35643.31  0.4509839  13642.85
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 9.

```


Fazendo modelos com as variáveis escolhidas no checkpoint anterior:

cola:

qnt_fornec - qtd_desp = 0.99
qnt_desp - tot_receita = 0.76
pes_jur - tot-receita = 0.85
pes_jur - tot_desp = 0.84
tot_receita - tot_desp = 0.99
qtd_doacoes - qtd_doadores = 1

com votos
tot_desp = 0.61
tot_receita = 0.6
pes_juridica = 0.55

escolha para modelo: 
tot_desp
qnt_forn
qtd_doadores

